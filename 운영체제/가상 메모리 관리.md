# Virtual Memory Management
* 가상 메모리(기억장치)
    * Non-continuous allocation
        * 사용자 프로그램을 block으로 분할하여 적재/실행
    * Paging/Segementation system
* 가상 메모리 관리의 목적
    * 가상 메모리 시스템 성능 최적화
        * cost model
        * 다양한 최적화 기법

# Cost Model for virtual Mem. Sys.
* Page fault frequency(발생빈도)
* Page fault rate(발생률)

* Page fault rate를 최소화 할 수 있도록 전략들을 설계해야 함
    * Context switch 및 kernel 개입을 최소화
    * 시스템 성능 향상

* Page reference string(d)
    * 프로세스의 수행 중 참조한 페이지 번호 순서
    * w = r1r2r3r4.....

# Hardware Components
* Address translation device(주소 사상 장치)
    * 주소 사상을 효율적으로 수행하기 위해 사용
* Bit Vectors
    * page 사용 상황에 대한 정보를 기록하는 비트들
    * Reference bits(used bit)
        * 참조 비트
    * Update bits(modified bits, write bits, dirty bits)
        * 갱신 비트

## Bit Vectors
* Refernce bit vector
    * 메모리에 적재된 각각의 page가 최근에 참조 되었는지를 표시
    * 운영
        1. 프로세스에 의해 참조되면 해당 page의 Ref. bit를 1로 설정
        2. 주기적으로 모든 reference bit를 0으로 초기화
    * Reference bit를 확인함으로서 최근에 참조된 page들을 확인 가능
* Update bit vector
    * Page가 메모리에 적재된 후, 프로세스에 의해 수정 되었는지를 표시
    * 주기적 초기화 없음
    * Update bit = 1
        * 해당 page의 main memory 상 내용 != swap device의 내용
        * 해당 page에 대한 write-back(to swap device)이 필요

# Software Components
## Allocation Strategies
* 각 프로세스에게 메모리를 얼마 만큼 줄 것인가?
    * Fixed allocation(고정 할당)
        * 프로세스의 실행 동안 고정된 크기의 메모리 할당
    * Variable allocation(가변 할당)
        * 프로세스의 실행 동안 할당하는 메모리의 크기가 유동적
* 고려사항
    * 프로세스 실행에 필요한 메모리 양을 예측해야 함
    * 너무 큰 메모리 할당
        * 메모리가 낭비 됨
    * 너무 적은 메모리 할당
        * Page fault rate가 증가한다
        * 시스템 성능 저하

## Fetch Strategis
* 특정 page를 메모리에 언제 적재할 것인가?
    * Demand fetch(demand paging)
        * 프로세스가 참조하는 페이지들만 적재
        * Page fault overhead
    * Anticipatory fetch(pre-paging)
        * 참조될 가능성이 높은 page 예측
        * 가까운 미래에 참조될 가능성이 높은 page를 미리 적재
        * 예측 성공 시, page fault overhead가 없음
        * prediction overhead, hit ratio에 민감함
* 실제 대부분의 시스템은 Demand fetch 기법 사용
    * 일반적으론 준수한 성능
    * Anticipatory fetch
        * Prediction overhead, 잘못된 예측 시 자원 낭비가 큼

## Replacement Strategis
* 새로운 page를 어떤 page와 교체 할 것인가?

## Cleaning Strategies
* 변경 된 page를 언제 write-back 할 것인가?
    * 변경된 내용을 swap device에 반영
    * Demand cleaning
        * 해당 page에 메모리에서 내려올 때 write-back
    * Anticipatory cleaning(pre-cleaning)
        * 더 이상 변경될 가능성이 없다고 판단할 때, 미리 write-back
        * Page 교체 시 발생하는 write-back 시간 절약
        * Write-back 이후, page 내용이 수정되면, overhead!
* 실제 대부분의 시스템은 Demand cleaning 기법 사용
    * 일반적으로 준수한 성능을 보여줌

## Load Control Strategies
* 시스템의 multi-programming degree 조절
    * Allocation strategies와 연계 됨
* 적정 수준의 multi-programming degree를 유지해야 함
    * Plateau(고원) 영역으로 유지
    * 저부하 상태(Under-loaded)
        * 시스템 자원 낭비, 성능 저하
    * 고부하 상태(Over-loaded)
        * 자원에 대한 경쟁 심화, 성능 저하
        * Thrashing(스레싱) 현상 발생
            * 과도한 page fault가 발생하는 형상


# Locality
* 프로세스가 프로그램/데이터의 특정 영역을 집중저긍로 참조하는 현상
* 공간적 지역성 
    * 참조한 영역과 인접한 영역을 참조하는 특성
* 시간적 지역성
    * 한 번 참조한 영역을 곧 다시 참조하는 특성


# Replacement Strategies
* Fixed allocation
* Variable allocation

## Fixed allocation
### Min Algorithm
* 앞으로 가장 오랫동안 참조하지 않을 page 교체
* 실현불가능함
    * page referene string을 알아야 한다
* 그래서 교체 기법의 성능 평가 도구로 활용됨


### Random Algorithm
* 무작위로 교체할 page 선택
* Low overhead
* No policy


### FIFO Algorithm
* 가장 먼저 들어온거 내보냄
* Page가 적재 된 시간을 기억하고 있어야 함
* 자주 사용되는 page가 교체 될 가능성이 높은
    * Locality에 대한 고려가 없다
* FIFO anomaly
    * 더 많은 page frame 을 할당받는데 page fault 많이날 수 있다
        * locality를 고려하지 않았기 때문이다


### LRU(Least Recently used) algorithm
* 가장 오랫동안 참조되지 않은 page를 교체
* Page 참조 시 마다 시간을 기록해야 함
* Locality에 기반을 둔 교체 기법
* Min algorithm에 근접한 성능을 보여줌
* 실제로 가장 많이 활용되는 기법
* 단점
    * 참조 시 마다 시간을 기록해야 함(Overhead)
        * 간소화된 정보 수집으로 해소 가능
    * Loop 실행에 필요한 크기보다 작은 수의 page frame이 할당 된 경우, page fault 수가 급격히 증가함
        * Allocation 기법에서 해결해야 하는 문제임

### LFU(Least Frequently Used) algorithm
* 가장 참조 횟수가 적은 Page를 교체
* page 참조 시마다, 참조 횟수를 누적시켜야 함
* Locality 활용
    * LRU 대비 적은 overhead
* 단점  
    * 최근 적재된 참조될 가능성이 높은 page가 교체 될 가능성이 있음
    * 참조 횟수 누적 overhead

### (NUR)Not Used Recently Algorithm
* LRU approximation scheme
    * LRU보다 적은 overhead로 비슷한 성능 달성 목적
* Bit vector 사용
    * Reference bit vector (r), Update bit vector(m)
* 교체순서
    1. (0, 0) 
    2. (0, 1)
    3. (1, 0)
    4. (1, 1)

### Clock Algorithm
* 주기적인 초기화 없음
    * Reference bit만 사용함
* Pointer를 돌리면서 교체 page 결정
    * 현재 가리키는 page의 reference bit가 0인경우 교체
    * 1인경우 reference bit 초기화 후 pointer 이동
* 먼저 적재된 page가 교체될 가능성이 높음
    * FIFO와 유사
* Reference bit를 사용하여 교체 페이지 결정
    * LRU와 유사

### Second Chance Algorithm
* Clock 알고리즘과 유사
* Update bit도 함께 고려함
    * 현재 가리키고 있는 page의 r, m 확인
    * 우선 reference 확인하고 1이면 0으로 변경
    * reference가 0이고 m이 1이면 0으로 변경 후 write-back list에 추가 후 이동
        * 이것이 write back을 해야한다는 것은 기록해둬야 하니까 그렇다

## Variable allocation
할당하는 페이지 프레임 수가 변한다
### Working Set algorithm
* Locality에 기반을 둔다
* working set
    * Process가 특정 시점에 자주 참조하는 page들의 집합
    * 최근 일정시간(델타)동안 참조된 page들의 집합
    * 시간에 따라 변함
    * W(t, 델타)
        * t - 델타, t 동안 참조된 page들의 집합
            * 참고로 포함이다
    * page fault rate 감소
    * 시스템 성능 향상
* Window 
    * 현재 바라봐야하는 영역
    * size는 고정
* Working set transition

* 성능평가
    * page fault 수 외 다른 지표도 함께 봐야 함
    * page frame도 고려해야 한다
* 특성
    * 적재 되는 page가 없더라도, 메모리를 반납하는 page가 있을 수 있다
    * 새로 적재되는 page가 있더라도, 교체 되는 page가 없을 수 있다
* 단점
    * Working set management overhead
        * 게속 관리해야 한다
    * Residence set(상주 집합)을 page fault가 없더라도, 지속적으로 관리해야 함

# Page Fault Frequency algorithm
* Residence set size를 page fault rate에 따라 결정
    * page fault가 나면 residence set을 바꾸자
    * Low page fault rate
        * 할당된 PF 감소
    * High page fault rate
        * process 에게 할당된 PF 수를 증가
* Resident set 갱신 및 메모리 할당
    * Page fault가 발생시에만 수행
    * Low overhead

* Algorithm
    1. page fault 발생 시, IFT 계산
        * IFT = tc - t(c-1)
            * 이전 PF(page fault) 발생 시간
            * 지금 PF 발생 시간
    2. IFT > 타우
        * tc-1 < t <= tc 동안 참조된 page들 만 유지
        * 나머지 page들은 메모리에서 내림
    3. IFT <= 타우
        * 기존 pages들 유지
        * 현재 참조된 page를 추가 적재
            * 메모리 할당(page frame)증가
* 성능평가
    * page frame과 page fault를 고려
* 특징
    * 메모리 상태 변화가 page fault 발생 시에만 변함
        * Low overhead


# Variable Min algorithm
* Optimal algorithm
    * 평균 메모리 할당량과 page fault 발생 횟수 모두 고려했을 때의 Optimal
* 실형 불가증한 기법
* Algorithm
    * Page r이 t 시간에 참조 되면, page r이 (t, t + 델타] 사이에 다시 참조되는지 확인
    * 참조 된다면 page r을 유지
    * 되지 않는다면 내림
* 최적 성능을 위한 delta 값은?
    * delta = R / U
        * R : page fault 발생 시 처리 비용
        * U : 한번의 참조 시간 동안 page를 메모리에 유지하는 비용
    

# Page Size
* 시스템 특성에 따라 다름
    * No best answer
    * 점점 커지는 경향
* 일반적인 page size
    * 2^7 bytes ~ 2^22 bytes
* Small page size
    * Large page table
        * High overhead(kernal)
    * 내부 단편화 감소
    * I/O 시간 증가
        * page에 여러개 올려야 함
    * Locality 향상
    * Page fault 증가
* Large page size
    * Small page table
        * Low overhead(kernal)
    * 내부 단편화 증가
    * I/O 시간 감소
        * 한번에 올림
    * Locality 저하
    * Page fault 감소
* 현대에는 CPU와 Memory 발전 -> page size큰게 좋다

# Program Restructuring
* 가상 메모리 시스템의 특성에 맞도록 프로그램을 재구성
* Example
    * Paging system, Page size = 1KB
    * sizeof(int) = 4bytes
    * int zar[256][256] -> 256 * 4 = 1KB
    * 접근 순서에 따라 page fault가 일어날 수 있다
* 사용자가 가상 메모리 관리 기법에 대해 이해하고 있다면, 프로그램의 구조를 변경하여 성능을 높일 수 있음

# TLB Reach
* TLB를 통해 접근 할 수 있는 메모리의 양
    * the number of entrites * the page size
* TLB의 hit ratio를 높이려면
    * TLB의 크기 증가
        * Expensive
    * Page 크기 증가 or 다양한 page size 지원
        * OS의 지원이 필요
            * 최근 OS의 발전 경향